{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gender Prediction with some meaningful Classifiers\n",
    "\n",
    "## The ML part of this code takes about 20 hours to train and learn providing about approximate 80% of accuracy in the test dataset which is the optimized accuracy. \n",
    "\n",
    "## The Neural network will take 5 Minutes and required to enable the GPU in kaggle accelerator\n",
    "\n",
    "## The rest of the code takes about 10 minutes to run excluding time for installing the libraries if not installed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!/opt/conda/bin/python3.7 -m pip install --upgrade pip\n",
    "!pip install pandas\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "!pip install numpy\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "!pip install sklearn\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "!pip install seaborn\n",
    "import seaborn as sn\n",
    "!pip install matplotlib\n",
    "from matplotlib import pyplot\n",
    "!pip install twython\n",
    "!pip install nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from collections import Counter\n",
    "!conda install -c asmeurer pattern -y\n",
    "from pattern.en import suggest\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-4349afcb85cf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'../input/twitter-user-gender-classification/gender-classifier-DFE-791531.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'ISO-8859-1'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mdf_discard\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "dataset = pd.read_csv('../input/twitter-user-gender-classification/gender-classifier-DFE-791531.csv', encoding = 'ISO-8859-1')\n",
    "dataset = pd.DataFrame(dataset)\n",
    "print(dataset.info())\n",
    "df_discard = dataset.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above data we can see that the GENDER column has 19953 non-null values, which we are using as a dependant variable!\n",
    "# Cleaning of the data we have\n",
    "Cleaning of dataset is important as we will have a better analysing of it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################################################# Data Cleaning\n",
    "dataset.drop(['profileimage', 'tweet_id', '_golden', 'gender_gold', 'profile_yn_gold', 'profile_yn:confidence', 'profile_yn', 'user_timezone', 'fav_number', '_unit_state', '_trusted_judgments', '_last_judgment_at', '_unit_id', 'name', 'tweet_created', 'tweet_coord', 'link_color', 'sidebar_color', 'tweet_location'], axis = 1, inplace = True)\n",
    "dataset.drop(dataset[(dataset['gender'] != 'male') & (dataset['gender'] != 'female') & (dataset['gender'] != 'brand')].index, inplace = True) ### manually dropping the non-null but irrelevent values\n",
    "print(dataset.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that in column GENDER we have just 18836 values inspite of 19953 non-null values in original dataset. This is becaues some of the values in GENDER column was \"nan\" and not null. so it had to be removed manually"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we will have to clean the text and description way of text representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################################################# Function for cleaning text & descriptions\n",
    "print(\"Text before formatting is applied to it is as \\n\", dataset['text'].head(5))\n",
    "print(\"Description before formatting is applied to it is as \\n\", dataset['description'].head(5))\n",
    "def cleaning(s):\n",
    "    s = str(s)\n",
    "    s = s.lower()\n",
    "    s = re.sub('\\s\\W',' ',s)\n",
    "    s = re.sub('\\W,\\s',' ',s)\n",
    "    s = re.sub(r'[^\\w]', ' ', s)\n",
    "    s = re.sub(\"\\d+\", \"\", s)\n",
    "    s = re.sub('\\s+',' ',s)\n",
    "    s = re.sub('[!@#$_]', '', s)\n",
    "    s = s.replace(\"co\",\"\")\n",
    "    s = s.replace(\"https\",\"\")\n",
    "    s = s.replace(\",\",\"\")\n",
    "    s = s.replace(\"[\\w*\",\" \")\n",
    "    return s\n",
    "dataset['text'] = [cleaning(s) for s in dataset['text']]\n",
    "print(\"\\nText after formatting is applied to it is as \\n\", dataset['text'].head(5))\n",
    "dataset['description'] = [cleaning(s) for s in dataset['description']]\n",
    "print(\"Description after formatting is applied to it is as \\n\", dataset['description'].head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "you get the idea of what is going no here in TEXT and DESCRIPTION cleaning.\n",
    "now it's time to convert dates into PANDS.DATETIME format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The date and time as stored in dataset is\\n\", dataset['created'].head(10))\n",
    "dataset['created'] = pd.to_datetime(dataset['created'])\n",
    "print(\"\\nThe date and time after conversions is\\n\", dataset['created'].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dividing the cleaned dataset into test & train datasets\n",
    "We will be dividing the dataset into Train & test datasets on the bases of GENDER:CONFIDANCE column value\n",
    "if GENDER:CONFIDANCE column value = 1 then the series goes to train set and tes set is whatever is lift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################################################# Function to make new index with ordered numeric values\n",
    "def correct_indexing(dataset):\n",
    "    dataset.insert(0, 'index', range(0, len(dataset)))\n",
    "    dataset.set_index(\"index\", inplace = True)\n",
    "    return\n",
    "\n",
    "dataset_store_gender_confodence_non1 = dataset.copy()\n",
    "dataset_store_gender_confodence_non1.drop(dataset_store_gender_confodence_non1[dataset_store_gender_confodence_non1['gender:confidence'] == 1].index, inplace = True)\n",
    "dataset.drop(dataset[(dataset['gender:confidence'] != 1)].index, inplace = True)\n",
    "dataset.drop(['gender:confidence'], axis = 1, inplace = True)\n",
    "dataset_store_gender_confodence_non1.drop(['gender:confidence'], axis = 1, inplace = True)\n",
    "correct_indexing(dataset)\n",
    "correct_indexing(dataset_store_gender_confodence_non1)\n",
    "print(\"####################################The Train subset of dataset is\\n\", dataset.info())\n",
    "print(\"####################################The Test subset of dataset is\\n\", dataset_store_gender_confodence_non1.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now we have both Training dataset and Testing dataset. we will be using Training dataset to find meaningfull data from in next sections!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Visualisations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot of gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.value_counts(dataset['gender']).plot(kind=\"pie\", startangle = 90, shadow = True, radius = 1.2, autopct = '%1.1f%%')\n",
    "pyplot.suptitle('Categorical Plotting of gender')\n",
    "pyplot.xlabel(\"\")\n",
    "pyplot.ylabel(\"\")\n",
    "pyplot.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems like there are almost equal percentages of\n",
    "genders and brands using twitter, we can predict that\n",
    "the brands can also have a equal percentage of gender\n",
    "running them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot of tweet popularity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.value_counts(dataset['created'].dt.year).sort_index().plot(kind=\"bar\")\n",
    "pyplot.suptitle('Categorical Plotting of Twitter popularity')\n",
    "pyplot.xlabel(\"Year\")\n",
    "pyplot.ylabel(\"Number of tweets\")\n",
    "pyplot.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting enough the popularity of twitter\n",
    "skyrocketed in 2009. The rest of the data shows a\n",
    "growth slope at angle close to 45degree and the boom\n",
    "in 2009 can be a hoax but we will verify it in other\n",
    "visualizations.\n",
    "Interestingly year 2010 was the least popular year\n",
    "for twitter as there was the least growth rate in past 6\n",
    "years from 2015. This fact can be confirmed in other\n",
    "graphs too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot of tweets per Hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.value_counts(dataset['created'].dt.hour).sort_index().plot(kind=\"bar\")\n",
    "pyplot.suptitle('Categorical Plotting of tweet war at what hour')\n",
    "pyplot.xlabel(\"Hour Time\")\n",
    "pyplot.ylabel(\"Number of tweets\")\n",
    "pyplot.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that what people do in their most\n",
    "productive hours, they tweet. This graph upon refine\n",
    "study shows a sharp rise in morning usage of twitter\n",
    "and the decline in late afternoon or evening is not that\n",
    "steep.\n",
    "Meanwhile we can make some guess as to what\n",
    "this sharp morning curve means, in the morning news is\n",
    "the most consumed media and this seems to be the\n",
    "pattern. Twitter is the major news source for people\n",
    "using it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot of words use per year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################################### Function to calculate the average based on 'column_dependent' &'column_working'\n",
    "def average(column_working, column_dependent, info):\n",
    "    word_avg_dependent = []\n",
    "    word_avg = []\n",
    "    add = 0\n",
    "    count = pd.value_counts(column_dependent)\n",
    "    for i in count.index:\n",
    "        for j in range (0, len(column_working)):\n",
    "            if (column_dependent[j] == i):\n",
    "                temp = column_working[j]\n",
    "                if (info == \"Text\"):\n",
    "                    pieces = len(temp.split())\n",
    "                if (info == \"Num\"):\n",
    "                    pieces = temp\n",
    "                add = add + int(pieces)\n",
    "        word_avg_dependent.append(i)\n",
    "        word_avg.append(add)\n",
    "        add = 0\n",
    "        pieces = 0\n",
    "    return word_avg_dependent, word_avg\n",
    "\n",
    "################################################################################################### Plot of words use per year\n",
    "word_avg_year, word_avg = average(dataset['description'] + \" \" + dataset['text'], dataset['created'].dt.year, \"Text\")\n",
    "pyplot.bar(word_avg_year, word_avg, tick_label = word_avg_year)\n",
    "pyplot.suptitle('Categorical Plotting of word use per year')\n",
    "pyplot.xlabel(\"Year\")\n",
    "pyplot.ylabel(\"Words used\")\n",
    "pyplot.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The words used per year shows some of the\n",
    "features which were seen in the popularity of twitter\n",
    "graph confirming the boom in 2009. The word count has\n",
    "increased per year in a very linear fashion.\n",
    "The increase rate seems to be slow in recent years,\n",
    "but this face can be verified in other graphs too.\n",
    "Interestingly the year 2010 is where the least words\n",
    "were used in past 6 years from 2015 confirming the\n",
    "2010 least growth rate fact."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical Plotting of word per tweet use per year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_avg_year, word_avg = average(dataset['description'] + \" \" + dataset['text'], dataset['created'].dt.year, \"Text\")\n",
    "years = pd.value_counts(dataset['created'].dt.year)\n",
    "i = 0\n",
    "for j in years:\n",
    "    word_avg[i] = word_avg[i] / j\n",
    "    i = i + 1\n",
    "pyplot.bar(word_avg_year, word_avg, tick_label = word_avg_year)\n",
    "pyplot.suptitle('Categorical Plotting of word per tweet use per year')\n",
    "pyplot.xlabel(\"Year\")\n",
    "pyplot.ylabel(\"Words used per tweet\")\n",
    "pyplot.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The words used in a tweet is declining and the\n",
    "decline per year rate is very slow. In span of 10 years\n",
    "the average word per tweet count drowned down from\n",
    "30 to 25.\n",
    "This seems to imply that people can say what they\n",
    "mean in less words, directly relating to increased\n",
    "educational qualities and quantities in the countries\n",
    "twitter is used mostly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical Plotting of text by gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_avg_gender, word_avg = average(dataset['text'], dataset['gender'], \"Text\")\n",
    "pyplot.pie(word_avg, labels = word_avg_gender, startangle = 90, shadow = True, radius = 1.2, autopct = '%1.1f%%') \n",
    "pyplot.suptitle('Categorical Plotting of text by gender')\n",
    "pyplot.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This graph Is in sync with gender graph and tells us\n",
    "that the amount of text accumulated in twitter\n",
    "database is equally contributed by male and females\n",
    "and brands"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical Plotting of tweet per gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_avg_gender, word_avg = average(dataset['tweet_count'], dataset['gender'], \"Num\")\n",
    "pyplot.pie(word_avg, labels = word_avg_gender, startangle = 90, shadow = True, radius = 1.2, autopct = '%1.1f%%') \n",
    "pyplot.suptitle('Categorical Plotting of tweet per gender')\n",
    "pyplot.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a interesting graph showing that the brands\n",
    "tweet almost as much as males and females combined.\n",
    "We can make many conclusions here. Looking at\n",
    "previous graphs one can say that the text weight of\n",
    "male and female tweet is double than that of brands.\n",
    "We can also conclude that the brands tweet mostly\n",
    "in the period of day when we discovered most activity\n",
    "on twitter. We will confirm these facts in other\n",
    "visualizations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical Plotting of retweet per year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_avg_year, word_avg = average(dataset['retweet_count'], dataset['created'].dt.year, \"Num\")\n",
    "pyplot.bar(word_avg_year, word_avg, tick_label = word_avg_year)\n",
    "pyplot.suptitle('Categorical Plotting of retweet per year')\n",
    "pyplot.xlabel(\"year\")\n",
    "pyplot.ylabel(\"Number of retweets\")\n",
    "pyplot.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a visualization with some shocking\n",
    "predictions. The popularity boom in 2009 can be seen to\n",
    "be due to massive shoot in retweet counts.\n",
    "This can be related to a certain world spread event\n",
    "which was gaining fire in twitter. But that is just a\n",
    "prediction.\n",
    "Removing 2009 the graph shows a very flattened\n",
    "curve with a slight rise every year rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical Plotting of retweet per gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_avg_gender, word_avg = average(dataset['retweet_count'], dataset['gender'], \"Num\")\n",
    "pyplot.pie(word_avg, labels = word_avg_gender, startangle = 90, shadow = True, radius = 1.2, autopct = '%1.1f%%') \n",
    "pyplot.suptitle('Categorical Plotting of retweet per gender')\n",
    "pyplot.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a visualization with some shocking\n",
    "predictions. The popularity boom in 2009 can be seen to\n",
    "be due to massive shoot in retweet counts.\n",
    "This can be related to a certain world spread event\n",
    "which was gaining fire in twitter. But that is just a\n",
    "prediction.\n",
    "Removing 2009 the graph shows a very flattened\n",
    "curve with a slight rise every year rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical Plotting of gender per hour tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################################## Function to plot line graph\n",
    "def plot_x_per_y(legend1, legend2, legend3, y_per_x1, y_per_x2, y_per_x3, column1, column2, column3, column4):\n",
    "    plot = pyplot.plot(y_per_x1[column1], [y for y in y_per_x1[column2]], label = legend1)\n",
    "    plot = pyplot.plot(y_per_x2[column1], [y for y in y_per_x2[column3]], label = legend2)\n",
    "    plot = pyplot.plot(y_per_x3[column1], [y for y in y_per_x3[column4]], label = legend3)\n",
    "    return plot\n",
    "\n",
    "################################################################################################## Function to Create temporary dataset as per the graph needs\n",
    "def x_per_y(y_check1, y_check2, y_check3, x, y, column1, column2, column3, column4):\n",
    "    x_now = pd.value_counts(x).sort_index()\n",
    "    a, b, c = 0, 0, 0\n",
    "    y_temp = []\n",
    "    y_per_x = pd.DataFrame(columns = [column1, column2, column3, column4])\n",
    "    for j in x_now.index:\n",
    "        y_temp = (y.loc[x == j])\n",
    "        for y_now in y_temp:\n",
    "            if y_now == y_check1:\n",
    "                a = a + 1\n",
    "            if y_now == y_check2:\n",
    "                b = b + 1\n",
    "            if y_now == y_check3:\n",
    "                c = c + 1\n",
    "        y_per_x.loc[j] = [j] + [a] + [b] + [c]\n",
    "        a, b, c = 0, 0, 0\n",
    "    plot_x_per_y(y_check1, y_check2, y_check3, y_per_x, y_per_x, y_per_x, column1, column2, column3, column4)\n",
    "    return\n",
    "\n",
    "################################################################################################## Categorical Plotting of gender per hour tweet\n",
    "plot = x_per_y( 'male', 'female', 'brand', dataset['created'].dt.hour, dataset['gender'], 'Hour', 'male', 'female', 'brand')\n",
    "plot = pyplot.suptitle('Categorical Plotting of gender per hour tweet')\n",
    "plot = pyplot.xlabel(\"Hour Time\")\n",
    "plot = pyplot.ylabel(\"Number of tweets\")\n",
    "plot = pyplot.legend()\n",
    "plot = pyplot.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how genders and brand behave in hourly\n",
    "manner in a day.\n",
    "This graph further confirms the fact that twitter is\n",
    "used as the news media for most of the user as the\n",
    "activity in the morning for all gender and brand is\n",
    "almost the same.\n",
    "Something else can also be predicted which is\n",
    "females use twitter the most in working hours!\n",
    "From 12 in night to 5 in morning are the quietest\n",
    "hours on twitter in a day."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical Plotting of gender per year tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = x_per_y( 'male', 'female', 'brand', dataset['created'].dt.year, dataset['gender'], 'Year', 'male', 'female', 'brand')\n",
    "plot = pyplot.suptitle('Categorical Plotting of gender per year tweet')\n",
    "plot = pyplot.xlabel(\"Year\")\n",
    "plot = pyplot.ylabel(\"Number of tweets\")\n",
    "plot = pyplot.legend()\n",
    "plot = pyplot.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This graph is yet another confirmation of the 2009\n",
    "popularity boom. As seen clearly the popularity was\n",
    "driven by male and females but brands don’t contribute\n",
    "in that popularity much. Further strengthening the fact\n",
    "that twitter is used to consume news media as brands\n",
    "usually use twitter for publishing media not consuming\n",
    "media.\n",
    "At the end of the graph there seems to be more\n",
    "female activity than male, and the trend is followed by\n",
    "brands too. We can predict this is due to recent gender\n",
    "equality that females in the field of business are now\n",
    "nor present full thn ever!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical Plotting of gender emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################################## Creating text per gender dataset\n",
    "text_male, text_female, text_brand = '', '', ''\n",
    "for i in dataset['text'].loc[dataset['gender'] == 'male'] + \" \" + dataset['description'].loc[dataset['gender'] == 'male']:\n",
    "    text_male = text_male + ' ' + i\n",
    "for i in dataset['text'].loc[dataset['gender'] == 'female'] + \" \" + dataset['description'].loc[dataset['gender'] == 'female']:\n",
    "    text_female = text_female + ' ' + i\n",
    "for i in dataset['text'].loc[dataset['gender'] == 'brand'] + \" \" + dataset['description'].loc[dataset['gender'] == 'brand']:\n",
    "    text_brand = text_brand + ' ' + i\n",
    "    \n",
    "################################################################################################## Function to convert any countable item into a dataset\n",
    "def to_dataset_converter(counter, column_name):\n",
    "    temp, temp2 = [], []\n",
    "    for i in counter.keys():\n",
    "        temp.append(i)\n",
    "    for i in counter.values():\n",
    "        temp2.append(i)\n",
    "    dataset_temp = pd.DataFrame(columns = [column_name, 'count'])\n",
    "    for i in range(0, len(counter)):\n",
    "        dataset_temp.loc[i] = [temp[i]] + [temp2[i]]\n",
    "    dataset_temp.sort_values(column_name, axis = 0, ascending = True, inplace = True, na_position ='last')\n",
    "    dataset_temp.drop(dataset_temp[dataset_temp[column_name] == 'compound'].index, inplace = True)\n",
    "    return dataset_temp\n",
    "\n",
    "################################################################################################## Function for emotional analysis per gender\n",
    "def emotional_analysis(text, column_name_1, column_name_2, column_name_3):\n",
    "    tokenize_words = word_tokenize(text)\n",
    "    clean_words=[]\n",
    "    for i in tokenize_words:\n",
    "        if i not in stopwords.words(\"english\"):\n",
    "            clean_words.append(i)\n",
    "    emotions = []\n",
    "    words_list = []\n",
    "    with open(\"../input/emotion/emotion.txt\",\"r\") as file:\n",
    "        for i in file:\n",
    "            temp = i.replace(\"\\n\",\"\")\n",
    "            temp = temp.strip()\n",
    "            temp = temp.replace(\" \",\"\")\n",
    "            temp = temp.replace(\",\",\"\")\n",
    "            temp = temp.replace(\"'\",\"\")\n",
    "            word, emotion = temp.split(\":\")\n",
    "            if word in clean_words:\n",
    "                emotions.append(emotion)\n",
    "                words_list.append(word)\n",
    "    counter_3 = Counter(words_list)\n",
    "    counter_2 = Counter(emotions)\n",
    "    counter_1 = SentimentIntensityAnalyzer().polarity_scores(text)\n",
    "    dataset_3 = to_dataset_converter(counter_3, column_name_3)\n",
    "    dataset_3.drop(dataset_3[(dataset_3['count'] == 1)].index, inplace = True)\n",
    "    dataset_2 = to_dataset_converter(counter_2, column_name_2)\n",
    "    dataset_1 = to_dataset_converter(counter_1, column_name_1)\n",
    "    return dataset_1, dataset_2, dataset_3\n",
    "\n",
    "################################################################################################## Gathering data for plotting graphs\n",
    "sentiments_dataset_male, emotions_dataset_male, words_dataset_male = emotional_analysis(text_male, 'sentiment' , 'emotion', 'words')\n",
    "sentiments_dataset_female, emotions_dataset_female, words_dataset_female = emotional_analysis(text_female, 'sentiment' , 'emotion', 'words')\n",
    "sentiments_dataset_brand, emotions_dataset_brand, words_dataset_brand = emotional_analysis(text_brand, 'sentiment' , 'emotion', 'words')\n",
    "\n",
    "################################################################################################## Categorical Plotting of gender emotion\n",
    "plot = plot_x_per_y('Emotion in males', 'Emotion in females', 'Emotion in brands', emotions_dataset_male, emotions_dataset_female, emotions_dataset_brand, 'emotion', 'count', 'count', 'count')\n",
    "plot = pyplot.suptitle('Categorical Plotting of gender emotion')\n",
    "plot = pyplot.xlabel(\"Emotion\")\n",
    "plot = pyplot.xticks(rotation = 90)\n",
    "plot = pyplot.ylabel(\"Proportions\")\n",
    "plot = pyplot.legend()\n",
    "plot = pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical Plotting of words of emotion used by males"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that the most common emotions\n",
    "used by both genders and brands are anger, attracted,\n",
    "fearful, happy and sad. Happy and sad being the most\n",
    "two prominent emotions expressed.\n",
    "Emotions like bored, average, cheated,\n",
    "embarrassed and lustful are not common among daily\n",
    "tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyplot.pie(words_dataset_male['count'], labels = words_dataset_male['words'], startangle = 90, shadow = True, radius = 1.2, autopct = '%1.1f%%') \n",
    "pyplot.suptitle('Categorical Plotting of words of emotion used by males')\n",
    "pyplot.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a graph which will be compared by the other\n",
    "two graphs shown below and it is very clear that the\n",
    "emotions expressed by males uses more different\n",
    "words than females or brands.\n",
    "Males require more words to convey same\n",
    "emotions than females.\n",
    "Providing some support to fact that females getting\n",
    "more into business and showing there presence in\n",
    "society as being more efficient in words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical Plotting of words of emotion used by females"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyplot.pie(words_dataset_female['count'], labels = words_dataset_female['words'], startangle = 90, shadow = True, radius = 1.2, autopct = '%1.1f%%') \n",
    "pyplot.suptitle('Categorical Plotting of words of emotion used by females')\n",
    "pyplot.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The words used by females to express emotions are\n",
    "less and consistent. This be an overall use of less words\n",
    "to convey a message and may have may significant\n",
    "predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical Plotting of words of emotion used by brands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyplot.pie(words_dataset_brand['count'], labels = words_dataset_brand['words'], startangle = 90, shadow = True, radius = 1.2, autopct = '%1.1f%%') \n",
    "pyplot.suptitle('Categorical Plotting of words of emotion used by brand')\n",
    "pyplot.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Brands use the least word to convey emotions and\n",
    "have high text in tweet ratio that genders which means\n",
    "that brands have a very low density of emotional words\n",
    "in a tweet than genders."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical Plotting of gender sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = plot_x_per_y('Sentiments in males', 'Sentiments in females', 'Sentiments in brands', sentiments_dataset_male, sentiments_dataset_female, sentiments_dataset_brand, 'sentiment', 'count', 'count', 'count')\n",
    "plot = pyplot.suptitle('Categorical Plotting of gender sentiment')\n",
    "plot = pyplot.xlabel(\"Sentiments\")\n",
    "plot = pyplot.ylabel(\"Proportion\")\n",
    "plot = plot = pyplot.legend()\n",
    "plot = pyplot.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The overall sentiment score of brands and genders\n",
    "is mostly neutral. The negative and positive ends only\n",
    "have minimum information. But still genders and\n",
    "brands tend to lean towards positive tweets rather than\n",
    "negative tweets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical Plotting of typing mistakes per gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "################################################################################################## Function to check if spelling is correct ot not\n",
    "def spelling_checker(text, breaker):\n",
    "    counter = 0\n",
    "    breaker_reach = 0\n",
    "    splits = text.split()\n",
    "    for split in splits:\n",
    "        word = re.compile(r\"(.)\\1{2,}\")\n",
    "        word_final = word.sub(r\"\\1\\1\", split)\n",
    "        correct_word = suggest(word_final)[0][0].replace(\"'\", \"\")\n",
    "        if correct_word != split:\n",
    "            counter += 1\n",
    "        breaker_reach += 1\n",
    "        if breaker == breaker_reach:\n",
    "            breaker_reach = 0\n",
    "            break\n",
    "    return counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################################## Collecting data of spelling mistakes per gender\n",
    "counter_male_typo = spelling_checker(text_male, 4000)\n",
    "counter_female_typo = spelling_checker(text_female, 4000)\n",
    "counter_brand_typo = spelling_checker(text_brand, 4000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################################## Categorical Plotting of typing mistakes per gender\n",
    "pyplot.bar(['male', 'female', 'brand'], [counter_male_typo, counter_female_typo, counter_brand_typo], tick_label = ['male', 'female', 'brand'])\n",
    "pyplot.suptitle('Categorical Plotting of typing mistakes per gender')\n",
    "pyplot.xlabel(\"Gender\")\n",
    "pyplot.ylabel(\"Typing mistakes\")\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you can see that females and brand make less\n",
    "typing mistakes than males. The sample size of this\n",
    "result was 4000 from each male, female and brands\n",
    "words.\n",
    "This shows that men are more de-focused or just\n",
    "type fast while making more mistakes. Other than that\n",
    "we can also conclude that the brands are run or at least\n",
    "their social accounts are handled mostly by females."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Classification Super Learner models deployment \"\"\"Prediction gender based on text\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-5a12d3644aa0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[1;31m####################################################################################################################### get database and plot the result of all models\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m \u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'gender'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'gender'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'male'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'female'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'brand'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'gender'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[0mmodels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_models\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "####################################################################################################################### Classification models deployment \"\"\"Prediction gender based on text\"\"\"\n",
    "####################################################################################################################### Stacking\n",
    "#######################################################################################################################\n",
    "####################################################################################################################### get the dataset\n",
    "file = open(r\"./Model Accuracy.txt\",\"w+\")\n",
    "\n",
    "def get_dataset(dataset_y, dataset_x):\n",
    "    y = dataset_y.values\n",
    "    count_vectorizer = CountVectorizer(max_features = 4000, stop_words = \"english\")\n",
    "    sparce_matrix = count_vectorizer.fit_transform(dataset_x).toarray()\n",
    "    X = sparce_matrix\n",
    "    return X, y\n",
    "\n",
    "####################################################################################################################### get a stacking ensemble of models\n",
    "def get_stacking():\n",
    "    ################################################################################################################### define the base models\n",
    "    level0 = list()\n",
    "    level0.append(('Logistic Regression', LogisticRegression()))\n",
    "    level0.append(('K Nearest Neighbour', KNeighborsClassifier()))\n",
    "    level0.append(('Decision Tree Classifier', DecisionTreeClassifier()))\n",
    "    level0.append(('Support Vector Classifier', SVC()))\n",
    "    level0.append(('Gaussian Navy Bayse', GaussianNB()))\n",
    "    level0.append(('ADA boost', AdaBoostClassifier()))\n",
    "    level0.append(('Bagging Classifier', BaggingClassifier(n_estimators = 10)))\n",
    "    level0.append(('Random Forest Classifier', RandomForestClassifier(n_estimators = 10)))\n",
    "    level0.append(('Extra Trees Classifier', ExtraTreesClassifier(n_estimators = 10)))\n",
    "    ################################################################################################################### define meta learner model\n",
    "    level1 = LogisticRegression()\n",
    "    ################################################################################################################### define the stacking ensemble\n",
    "    model = StackingClassifier(estimators = level0, final_estimator = level1, cv = 5)\n",
    "    return model\n",
    "\n",
    "####################################################################################################################### get a list of models to evaluate\n",
    "def get_models():\n",
    "    models = dict()\n",
    "    models['Logistic Regression'] = LogisticRegression()\n",
    "    models['K Nearest Neighbour'] = KNeighborsClassifier()\n",
    "    models['Decision Tree Classifier'] = DecisionTreeClassifier()\n",
    "    models['Support Vector Classifier'] = SVC()\n",
    "    models['Gaussian Navy Bayse'] = GaussianNB()\n",
    "    models['ADA boost'] = AdaBoostClassifier()\n",
    "    models['Bagging Classifier'] = BaggingClassifier()\n",
    "    models['Random Forest Classifier'] = RandomForestClassifier()\n",
    "    models['Extra Trees Classifier'] = ExtraTreesClassifier()\n",
    "    models['Stacking'] = get_stacking()\n",
    "    return models\n",
    "\n",
    "####################################################################################################################### evaluate a give model using cross-validation\n",
    "def evaluate_model(model, X, y):\n",
    "    cv = RepeatedStratifiedKFold(n_splits = 10, n_repeats = 3, random_state = 1)\n",
    "    scores = cross_val_score(model, X, y, scoring = 'accuracy', cv = cv, n_jobs = -1, error_score = 'raise')\n",
    "    return scores\n",
    "\n",
    "####################################################################################################################### get database and plot the result of all models \n",
    "dataset['gender'] = dataset['gender'].replace(['male', 'female', 'brand'],[0, 1, 2])\n",
    "X, y = get_dataset(dataset['gender'], dataset['text'])\n",
    "models = get_models()\n",
    "results, names = list(), list()\n",
    "for name, model in models.items():\n",
    "    scores = evaluate_model(model, X, y)\n",
    "    results.append(scores)\n",
    "    names.append(name)\n",
    "    file.writelines(str('>%s %.3f (%.3f)' % (name, mean(scores), std(scores)) + \"\\n\"))\n",
    "print(file)\n",
    "pyplot.boxplot(results, labels = names, showmeans = True)\n",
    "pyplot.xticks(rotation = 90)\n",
    "pyplot.show()\n",
    "\n",
    "####################################################################################################################### predicting on new dataset\n",
    "model_test = get_stacking()\n",
    "dataset_store_gender_confodence_non1['gender'] = dataset_store_gender_confodence_non1['gender'].replace(['male', 'female', 'brand'],[0, 1, 2])\n",
    "X_test, y_test = get_dataset(dataset_store_gender_confodence_non1['gender'], dataset_store_gender_confodence_non1['text'])\n",
    "model_test.fit(X_test, y_test)\n",
    "scores_test = evaluate_model(model_test, X_test, y_test)\n",
    "print('>%s %.3f (%.3f)' % ('stacking_pridection', mean(scores_test), std(scores_test)))\n",
    "yhat = model_test.predict(X_test)\n",
    "\n",
    "confusion_test = confusion_matrix(y_test , yhat)\n",
    "\n",
    "def accuracy(confusion_matrix):\n",
    "    diagonal_sum = confusion_matrix.trace()\n",
    "    sum_of_all_elements = confusion_matrix.sum()\n",
    "    return diagonal_sum / sum_of_all_elements\n",
    "\n",
    "file.writelines(\"Accuracy of super learner stacking ML algorithm on test dataset is :-> \" + str(accuracy(confusion_test) * 100))\n",
    "sn.set(font_scale = 1.4)\n",
    "sn.heatmap(confusion_test, annot = True, annot_kws = {\"size\" : 16}, fmt = \"d\", xticklabels = ['male', 'female', 'brand'], yticklabels = ['male', 'female', 'brand'])\n",
    "pyplot.xlabel('Predictions')\n",
    "pyplot.ylabel('Actual')\n",
    "pyplot.savefig(\"Confusion Matrix of the Ensemble learner model.png\", dpi = 300, bbox_inches = 'tight')\n",
    "pyplot.show()\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network for prediction of gender based on text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below will get the tokenized sentences form the text and descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################## Get sequences of tokenised \n",
    "def get_sequences(texts, vocab_length):\n",
    "    tokenizer = Tokenizer(num_words = vocab_length)\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "    sequences = tokenizer.texts_to_sequences(texts)\n",
    "    max_seq_length = np.max([len(sequence) for sequence in sequences])\n",
    "    sequences = pad_sequences(sequences, maxlen = max_seq_length, padding = 'post')\n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################## Get RGB values from the hexadecimal values\n",
    "def hex_to_decimal(x):\n",
    "    try:\n",
    "        return np.int(x, 16)\n",
    "    except:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################## Get RGB values from The column\n",
    "def get_rgb(colors):\n",
    "    r = colors.apply(lambda x: hex_to_decimal(x[0:2]))\n",
    "    g = colors.apply(lambda x: hex_to_decimal(x[2:4]))\n",
    "    b = colors.apply(lambda x: hex_to_decimal(x[4:6]))\n",
    "    return r, g, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################## Prepairing complete dataset\n",
    "def dataset_prepairing():\n",
    "    df = df_discard.copy()\n",
    "    df = df.drop(['_unit_id', 'name', 'profileimage', 'tweet_id'], axis = 1)\n",
    "    \n",
    "    missing_cols = df.columns[df.isna().mean() > 0.3]\n",
    "    df = df.drop(missing_cols, axis = 1)\n",
    "    \n",
    "    df.drop(df[(df['gender'] != 'male') & (df['gender'] != 'female') & (df['gender'] != 'brand')].index, inplace = True)\n",
    "    df.drop(df[(df['gender:confidence'] < 0.7)].index, inplace = True)\n",
    "    \n",
    "    judgment_nas = df[df['_last_judgment_at'].isna()].index\n",
    "    df = df.drop(judgment_nas, axis = 0).reset_index(drop = True)\n",
    "    df['description'] = df['description'].fillna('')\n",
    "    for column in ['_last_judgment_at', 'created', 'tweet_created']:\n",
    "        df[column] = pd.to_datetime(df[column])\n",
    "    \n",
    "    df['judgment_day'] = df['_last_judgment_at'].apply(lambda x: x.day)\n",
    "    df['judgment_hour'] = df['_last_judgment_at'].apply(lambda x: x.hour)\n",
    "    \n",
    "    df['created_year'] = df['created'].apply(lambda x: x.year)\n",
    "    df['created_month'] = df['created'].apply(lambda x: x.month)\n",
    "    df['created_day'] = df['created'].apply(lambda x: x.day)\n",
    "    df['created_hour'] = df['created'].apply(lambda x: x.hour)\n",
    "    \n",
    "    df['tweet_hour'] = df['tweet_created'].apply(lambda x: x.hour)\n",
    "    \n",
    "    df = df.drop(['_last_judgment_at', 'created', 'tweet_created'], axis = 1)\n",
    "    desc = get_sequences(df['description'], vocab_length = 20000)\n",
    "    tweets = get_sequences(df['text'], vocab_length = 20000)\n",
    "    \n",
    "    df = df.drop(['description', 'text'], axis = 1)\n",
    "    df = df.drop(['_golden', '_unit_state', '_trusted_judgments', 'profile_yn'], axis = 1)\n",
    "    df['link_red'], df['link_green'], df['link_blue'] = get_rgb(df['link_color'])\n",
    "    df['side_red'], df['side_green'], df['side_blue'] = get_rgb(df['sidebar_color'])\n",
    "    \n",
    "    df = df.drop(['link_color', 'sidebar_color'], axis = 1)\n",
    "    label_mapping = {'female': 0, 'male': 1, 'brand': 2}\n",
    "    df['gender'] = df['gender'].replace(label_mapping)\n",
    "    y = df['gender'].copy()\n",
    "    X = df.drop('gender', axis = 1).copy()\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X = pd.DataFrame(scaler.fit_transform(X), columns = X.columns)\n",
    "    return desc, tweets, X, y\n",
    "\n",
    "desc, tweets, X, y = dataset_prepairing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################## Prepairing test dataset\n",
    "X_train, X_test, desc_train, desc_test, tweets_train, tweets_test, y_train, y_test = train_test_split(X, desc, tweets, y, train_size = 0.7, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making of the Neural network model for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################## Function to load model\n",
    "def build_model():\n",
    "    X_inputs = tf.keras.Input(shape=(X.shape[1],))\n",
    "    desc_inputs = tf.keras.Input(shape = (desc.shape[1], ))\n",
    "    tweet_inputs = tf.keras.Input(shape = (tweets.shape[1], ))\n",
    "    \n",
    "    # X\n",
    "    X_dense1 = tf.keras.layers.Dense(256, activation='relu')(X_inputs)\n",
    "    X_dense2 = tf.keras.layers.Dense(256, activation='relu')(X_dense1)\n",
    "\n",
    "    # desc\n",
    "    desc_embedding = tf.keras.layers.Embedding(input_dim = 20000, output_dim = 256, input_length = desc.shape[1])(desc_inputs)\n",
    "    desc_gru = tf.keras.layers.GRU(256, return_sequences = False)(desc_embedding)\n",
    "    desc_flatten = tf.keras.layers.Flatten()(desc_embedding)\n",
    "    desc_concat = tf.keras.layers.concatenate([desc_gru, desc_flatten])\n",
    "\n",
    "    # tweets\n",
    "    tweet_embedding = tf.keras.layers.Embedding(input_dim = 20000, output_dim = 256, input_length = tweets.shape[1])(tweet_inputs)\n",
    "    tweet_gru = tf.keras.layers.GRU(256, return_sequences = False)(tweet_embedding)\n",
    "    tweet_flatten = tf.keras.layers.Flatten()(tweet_embedding)\n",
    "    tweet_concat = tf.keras.layers.concatenate([tweet_gru, tweet_flatten])\n",
    "\n",
    "    concat = tf.keras.layers.concatenate([X_dense2, desc_concat, tweet_concat])\n",
    "\n",
    "    outputs = tf.keras.layers.Dense(3, activation='softmax')(concat)\n",
    "\n",
    "    model = tf.keras.Model(inputs = [X_inputs, desc_inputs, tweet_inputs], outputs = outputs)\n",
    "    return model\n",
    "model = build_model()\n",
    "print(model.summary())\n",
    "tf.keras.utils.plot_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################## Training the model\n",
    "model.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])\n",
    "batch_size = 16\n",
    "epochs = 3\n",
    "history = model.fit([X_train, desc_train, tweets_train], y_train, validation_split = 0.2, batch_size = batch_size, epochs = epochs, callbacks = [tf.keras.callbacks.ModelCheckpoint('./model.h5', save_best_only = True, save_weights_only = True), tf.keras.callbacks.ReduceLROnPlateau()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################## Predicting with the model\n",
    "model.load_weights('./model.h5')\n",
    "results = model.evaluate([X_test, desc_test, tweets_test], y_test, verbose = 0)\n",
    "print(\"Model Accuracy: {:.2f}%\".format(results[1] * 100))\n",
    "y_true = np.array(y_test)\n",
    "\n",
    "y_pred = model.predict([X_test, desc_test, tweets_test])\n",
    "y_pred = map(lambda x: np.argmax(x), y_pred)\n",
    "y_pred = np.array(list(y_pred))\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "clr = classification_report(y_true, y_pred, target_names = ['Female', 'Male', 'Brand'])\n",
    "pyplot.figure(figsize = (6, 6))\n",
    "sn.heatmap(cm, annot = True, fmt = 'g', cbar = False, cmap = 'Blues')\n",
    "pyplot.xticks(np.arange(3) + 0.5, ['Female', 'Male', 'Brand'])\n",
    "pyplot.yticks(np.arange(3) + 0.5, ['Female', 'Male', 'Brand'])\n",
    "pyplot.xlabel(\"Predicted\")\n",
    "pyplot.ylabel(\"Actual\")\n",
    "pyplot.title(\"Confusion Matrix\")\n",
    "pyplot.savefig(\"Confusion matrix.png\", dpi = 300, bbox_inches = 'tight')\n",
    "pyplot.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
